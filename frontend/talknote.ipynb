{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3085190,"sourceType":"datasetVersion","datasetId":1877225}],"dockerImageVersionId":31236,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q openai-whisper speechbrain torchaudio librosa scikit-learn","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-09T08:01:59.552667Z","iopub.execute_input":"2026-01-09T08:01:59.553194Z","iopub.status.idle":"2026-01-09T08:02:12.413041Z","shell.execute_reply.started":"2026-01-09T08:01:59.553166Z","shell.execute_reply":"2026-01-09T08:02:12.412324Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m864.1/864.1 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m788.2/788.2 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport librosa\nimport torch\nimport torchaudio\nimport numpy as np\nfrom collections import defaultdict\nimport whisper\nfrom speechbrain.pretrained import EncoderClassifier\nfrom sklearn.cluster import SpectralClustering\nfrom sklearn.metrics import silhouette_score\n\nSAMPLE_RATE = 16000\nASR_CHUNK_SEC = 10.0\n\nCHUNK_SAMPLES = int(ASR_CHUNK_SEC * SAMPLE_RATE)\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nprint(\"Device:\", DEVICE)\nprint(\"Chunk samples:\", CHUNK_SAMPLES)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T08:02:12.414644Z","iopub.execute_input":"2026-01-09T08:02:12.414898Z","iopub.status.idle":"2026-01-09T08:02:28.656397Z","shell.execute_reply.started":"2026-01-09T08:02:12.414870Z","shell.execute_reply":"2026-01-09T08:02:28.655727Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/speechbrain/utils/torch_audio_backend.py:57: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n  available_backends = torchaudio.list_audio_backends()\n/tmp/ipykernel_55/2233375752.py:8: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n  from speechbrain.pretrained import EncoderClassifier\n","output_type":"stream"},{"name":"stdout","text":"Device: cuda\nChunk samples: 160000\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"class TalkNotePipeline:\n    def __init__(self, session_id: str, run_dir: str, sample_rate: int = 16000, speaker_estimator=None):\n        print(\"[LOG] Initializing TalkNotePipeline\")\n\n        self.session_id = session_id\n        self.sample_rate = sample_rate\n        self.run_dir = run_dir\n        os.makedirs(self.run_dir, exist_ok=True)\n\n        self.total_audio_sec = 0.0\n        self.audio_offset_sec = 0.0\n\n        self.audio = None\n        self.asr_segments = []\n        self.embeddings = []\n\n        self.aligned_segments = []\n        self.merged_segments = []\n        self.final_transcript = []\n\n        self.memory_bank = {}\n        self.speaker_groups = {}\n\n        self.user_transcript = []\n\n        self.speaker_estimator = speaker_estimator\n\n        print(f\"[LOG] Initialization complete | session={self.session_id}\")\n        \n    def load_audio(self, audio_chunk: np.ndarray):\n        if audio_chunk is None or len(audio_chunk) == 0:\n            self.audio = None\n            return\n    \n        if audio_chunk.dtype != np.float32:\n            audio_chunk = audio_chunk.astype(np.float32)\n    \n        self.audio = audio_chunk\n        self.audio_offset_sec = self.total_audio_sec\n    \n        chunk_duration = len(audio_chunk) / self.sample_rate\n        self.total_audio_sec += chunk_duration\n    \n        print(\n            f\"[LOG] Audio chunk loaded | \"\n            f\"samples={len(audio_chunk)} | \"\n            f\"offset={self.audio_offset_sec:.2f}s\"\n        )\n\n    def run_asr(self, whisper_model):\n        print(\"[LOG] Entering run_asr\")\n    \n        if self.audio is None or len(self.audio) < self.sample_rate:\n            self.asr_segments = []\n            print(\"[LOG] ASR skipped (audio too short)\")\n            return\n    \n        result = whisper_model.transcribe(\n            self.audio,\n            beam_size=1,\n            verbose=False\n        )\n    \n        self.asr_segments = [\n            {\n                \"start\": seg[\"start\"] + self.audio_offset_sec,\n                \"end\": seg[\"end\"] + self.audio_offset_sec,\n                \"text\": seg[\"text\"].strip()\n            }\n            for seg in result.get(\"segments\", [])\n        ]\n    \n        print(f\"[LOG] ASR batch completed | segments={len(self.asr_segments)}\")\n\n    def extract_embeddings(self, embedder):\n        print(\"[LOG] Entering extract_embeddings\")\n    \n        if self.audio is None:\n            return\n    \n        window_len = int(self.sample_rate * 10)\n        stride = int(self.sample_rate * 5)\n    \n        audio_tensor = torch.from_numpy(self.audio).to(embedder.device)\n    \n        batch_embeddings = []\n\n        for start in range(0, max(1, len(self.audio) - window_len + 1), stride):\n            window = audio_tensor[start:start + window_len]\n            if window.shape[0] < window_len:\n                continue\n    \n            window = window.unsqueeze(0)\n    \n            with torch.no_grad():\n                emb = embedder.encode_batch(window)[0].squeeze(0).cpu().numpy()\n    \n            emb = emb / np.linalg.norm(emb)\n    \n            batch_embeddings.append({\n                \"start\": self.audio_offset_sec + start / self.sample_rate,\n                \"end\": self.audio_offset_sec + (start + window_len) / self.sample_rate,\n                \"embedding\": emb,\n            })\n    \n        if hasattr(self, \"speaker_model_frozen\") and self.speaker_model_frozen:\n            for emb in batch_embeddings:\n                self.assign_embedding_to_speaker(emb)\n    \n        self.embeddings.extend(batch_embeddings)\n        self.embeddings.sort(key=lambda x: x[\"start\"])\n    \n        print(f\"[LOG] Embeddings appended | total={len(self.embeddings)}\")\n\n    def cosine(self, a, b):\n        return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n\n    def cluster_speakers(self, min_embeddings: int = 16):\n        print(\"[LOG] Entering cluster_speakers (warm-up mode)\")\n    \n        if self.speaker_estimator is None:\n            raise RuntimeError(\n                \"speaker_estimator not provided to TalkNotePipeline\"\n            )\n    \n        if hasattr(self, \"speaker_model_frozen\") and self.speaker_model_frozen:\n            print(\"[LOG] Speaker model frozen — skipping clustering\")\n            return\n    \n        if len(self.embeddings) < min_embeddings:\n            print(\"[LOG] Insufficient embeddings for initial clustering\")\n            return\n    \n        X = np.stack([e[\"embedding\"] for e in self.embeddings])\n    \n        n_speakers = self.speaker_estimator(X)\n        print(f\"[LOG] Initial speaker count estimated: {n_speakers}\")\n    \n        labels = SpectralClustering(\n            n_clusters=n_speakers,\n            affinity=\"nearest_neighbors\",\n            assign_labels=\"kmeans\",\n            random_state=42,\n        ).fit_predict(X)\n    \n        for emb, label in zip(self.embeddings, labels):\n            emb[\"cluster\"] = int(label)\n    \n        for emb in self.embeddings:\n            spk_id = f\"SPEAKER_{emb['cluster']}\"\n            if spk_id not in self.memory_bank:\n                self.memory_bank[spk_id] = {\n                    \"centroid\": emb[\"embedding\"],\n                    \"count\": 1,\n                    \"sim_history\": [],\n                }\n    \n        self.speaker_model_frozen = True\n        print(\"[LOG] Speaker clustering frozen\")\n\n    def align_segments(self):\n        print(\"[LOG] Entering align_segments\")\n\n        self.aligned_segments = []\n\n        if not self.asr_segments:\n            print(\"[LOG] No ASR segments to align\")\n            return\n    \n        if not self.embeddings:\n            print(\"[LOG] No embeddings available for alignment\")\n            return\n    \n        if not self.memory_bank:\n            print(\"[LOG] Memory bank empty — cannot assign speakers\")\n            return\n    \n        dropped_segments = 0\n    \n        for seg in self.asr_segments:\n            best_score = -1.0\n            best_speaker = None\n    \n            seg_start = seg[\"start\"]\n            seg_end = seg[\"end\"]\n    \n            for emb in self.embeddings:\n                overlap = min(seg_end, emb[\"end\"]) - max(seg_start, emb[\"start\"])\n                if overlap <= 0:\n                    continue\n    \n                if \"cluster\" not in emb:\n                    continue\n    \n                spk_id = f\"SPEAKER_{emb['cluster']}\"\n    \n                if spk_id not in self.memory_bank:\n                    continue\n    \n                centroid = self.memory_bank[spk_id][\"centroid\"]\n    \n                sim = float(np.dot(centroid, emb[\"embedding\"]))\n    \n                score = overlap * sim\n    \n                if score > best_score:\n                    best_score = score\n                    best_speaker = spk_id\n    \n            if best_speaker is None:\n                dropped_segments += 1\n                continue\n    \n            self.aligned_segments.append({\n                \"start\": seg_start,\n                \"end\": seg_end,\n                \"speaker\": best_speaker,\n                \"text\": seg[\"text\"],\n            })\n    \n        print(\n            f\"[LOG] Alignment completed | \"\n            f\"aligned={len(self.aligned_segments)} | \"\n            f\"dropped={dropped_segments}\"\n        )\n\n    def update_memory_bank(self):\n        print(\"[LOG] Entering update_memory_bank\")\n\n        if not hasattr(self, \"speaker_model_frozen\") or not self.speaker_model_frozen:\n            print(\"[LOG] Speaker model not frozen — skipping memory update\")\n            return\n    \n        if not hasattr(self, \"last_memory_update_idx\"):\n            self.last_memory_update_idx = 0\n\n        new_embeddings = self.embeddings[self.last_memory_update_idx :]\n    \n        for emb in new_embeddings:\n            if \"cluster\" not in emb:\n                continue\n            \n            speaker_id = f\"SPEAKER_{emb['cluster']}\"\n            vec = emb[\"embedding\"]\n    \n            if speaker_id not in self.memory_bank:\n                self.memory_bank[speaker_id] = {\n                    \"centroid\": vec,\n                    \"count\": 1,\n                    \"sim_history\": []\n                }\n                continue\n    \n            old = self.memory_bank[speaker_id][\"centroid\"]\n            sim = float(np.dot(old, vec))\n    \n            alpha = 0.1 + 0.4 * max(sim, 0)\n            new = (1 - alpha) * old + alpha * vec\n            new /= np.linalg.norm(new)\n    \n            self.memory_bank[speaker_id][\"centroid\"] = new\n            self.memory_bank[speaker_id][\"count\"] += 1\n            self.memory_bank[speaker_id][\"sim_history\"].append(sim)\n    \n        self.last_memory_update_idx = len(self.embeddings)\n    \n        print(\"[LOG] Memory bank updated | speakers:\", len(self.memory_bank))\n\n    def deduplicate_text(self, text):\n        words = text.split()\n        deduped = []\n        for w in words:\n            if not deduped or deduped[-1] != w:\n                deduped.append(w)\n        return \" \".join(deduped)\n\n    def merge_segments(self, finalize_before_sec: float):\n        print(\"[LOG] Entering merge_segments\")\n    \n        if not self.aligned_segments:\n            return\n    \n        self.aligned_segments.sort(key=lambda x: x[\"start\"])\n    \n        merged = []\n        cur = self.aligned_segments[0]\n    \n        for seg in self.aligned_segments[1:]:\n            gap = seg[\"start\"] - cur[\"end\"]\n    \n            if seg[\"speaker\"] == cur[\"speaker\"] and gap < 1.0:\n                cur[\"end\"] = max(cur[\"end\"], seg[\"end\"])\n                cur[\"text\"] += \" \" + seg[\"text\"]\n            else:\n                merged.append(cur)\n                cur = seg\n    \n        merged.append(cur)\n    \n        finalized = []\n        remaining = []\n    \n        for seg in merged:\n            if seg[\"end\"] <= finalize_before_sec:\n                seg[\"text\"] = self.deduplicate_text(seg[\"text\"])\n                finalized.append(seg)\n            else:\n                remaining.append(seg)\n    \n        self.final_transcript.extend(finalized)\n        self.aligned_segments = remaining\n    \n        if not hasattr(self, \"finalized_until_sec\"):\n            self.finalized_until_sec = 0.0\n    \n        self.finalized_until_sec = max(self.finalized_until_sec, finalize_before_sec)\n    \n        print(\n            f\"[LOG] Segments finalized={len(finalized)} | \"\n            f\"remaining={len(remaining)}\"\n        )\n\n    def _collect_speaker_stats(self):\n        stats = {}\n    \n        for seg in self.final_transcript:\n            spk = seg[\"speaker\"]\n            stats.setdefault(spk, {\n                \"total_time\": 0.0,\n                \"segments\": []\n            })\n            stats[spk][\"total_time\"] += seg[\"end\"] - seg[\"start\"]\n            stats[spk][\"segments\"].append(seg)\n    \n        return stats\n\n    def _should_merge(self, spk_a, spk_b, stats):\n        mem = self.memory_bank\n    \n        dur_a = stats[spk_a][\"total_time\"]\n        dur_b = stats[spk_b][\"total_time\"]\n    \n        if min(dur_a, dur_b) < 3.0:\n            return False\n    \n        sim = self.cosine(mem[spk_a][\"centroid\"], mem[spk_b][\"centroid\"])\n    \n        if sim < 0.85:\n            return False\n    \n        for seg_a in stats[spk_a][\"segments\"]:\n            for seg_b in stats[spk_b][\"segments\"]:\n                if max(seg_a[\"start\"], seg_b[\"start\"]) < min(seg_a[\"end\"], seg_b[\"end\"]):\n                    return False\n    \n        dominance_ratio = min(dur_a, dur_b) / max(dur_a, dur_b)\n        if dominance_ratio > 0.6:\n            return False\n    \n        return True\n\n    def collapse_speakers(self, min_interval_sec: float = 30.0):\n        print(\"[LOG] Entering collapse_speakers\")\n\n        if not self.final_transcript:\n            return\n    \n        if not hasattr(self, \"last_collapse_time_sec\"):\n            self.last_collapse_time_sec = 0.0\n    \n        if self.total_audio_sec - self.last_collapse_time_sec < min_interval_sec:\n            print(\"[LOG] Skipping collapse (interval gate)\")\n            return\n    \n        if not self.final_transcript:\n            return\n    \n        exposed_speakers = {\n            seg[\"speaker\"]\n            for seg in self.user_transcript\n            if \"speaker\" in seg\n        }\n    \n        stats = self._collect_speaker_stats()\n        speakers = list(stats.keys())\n        parent = {s: s for s in speakers}\n    \n        def find(x):\n            while parent[x] != x:\n                parent[x] = parent[parent[x]]\n                x = parent[x]\n            return x\n    \n        def union(a, b):\n            ra, rb = find(a), find(b)\n            if ra != rb:\n                parent[rb] = ra\n    \n        for i in range(len(speakers)):\n            for j in range(i + 1, len(speakers)):\n                a, b = speakers[i], speakers[j]\n    \n                if a in exposed_speakers or b in exposed_speakers:\n                    continue\n    \n                if self._should_merge(a, b, stats):\n                    union(a, b)\n    \n        collapse_map = {}\n        for s in speakers:\n            root = find(s)\n            collapse_map.setdefault(root, []).append(s)\n    \n        self.speaker_groups = collapse_map\n        self.last_collapse_time_sec = self.total_audio_sec\n    \n        print(f\"[LOG] Speaker groups stabilized: {self.speaker_groups}\")\n\n    def apply_speaker_collapse(self):\n        if not self.speaker_groups:\n            return\n    \n        if not hasattr(self, \"speaker_to_person\"):\n            self.speaker_to_person = {}\n            self.next_person_id = 0\n    \n        for seg in self.user_transcript:\n            spk = seg.get(\"speaker\")\n            pid = seg.get(\"person\")\n            if spk is not None and pid is not None:\n                self.speaker_to_person[spk] = pid\n                try:\n                    self.next_person_id = max(\n                        self.next_person_id,\n                        int(pid.split(\"_\")[-1]) + 1\n                    )\n                except Exception:\n                    pass\n    \n        for group in self.speaker_groups.values():\n            existing_pid = None\n    \n            for spk in group:\n                if spk in self.speaker_to_person:\n                    existing_pid = self.speaker_to_person[spk]\n                    break\n    \n            if existing_pid is None:\n                existing_pid = f\"PERSON_{self.next_person_id}\"\n                self.next_person_id += 1\n    \n            for spk in group:\n                self.speaker_to_person[spk] = existing_pid\n    \n        for seg in self.final_transcript:\n            seg[\"person\"] = self.speaker_to_person.get(seg[\"speaker\"])\n\n    def assign_embedding_to_speaker(self, emb):\n        best_spk = None\n        best_sim = -1.0\n    \n        for spk_id, mem in self.memory_bank.items():\n            sim = float(np.dot(mem[\"centroid\"], emb[\"embedding\"]))\n            if sim > best_sim:\n                best_sim = sim\n                best_spk = spk_id\n    \n        emb[\"cluster\"] = int(best_spk.split(\"_\")[-1])\n\n    def build_user_transcript(self, force_rebuild: bool = False):\n        if force_rebuild:\n            print(\"[LOG] Rebuilding user transcript from scratch\")\n            self.user_transcript = []\n            self.last_emitted_end_sec = -1.0\n    \n        if not hasattr(self, \"last_emitted_end_sec\"):\n            self.last_emitted_end_sec = -1.0\n    \n        if not self.final_transcript:\n            return\n    \n        new_segments = []\n    \n        for seg in self.final_transcript:\n            if not force_rebuild and seg[\"end\"] <= self.last_emitted_end_sec:\n                continue\n    \n            person = seg.get(\"person\")\n            if person is None:\n                continue\n    \n            new_segments.append({\n                \"person\": person,\n                \"start\": seg[\"start\"],\n                \"end\": seg[\"end\"],\n                \"text\": self.deduplicate_text(seg[\"text\"]),\n            })\n    \n        if not new_segments:\n            return\n    \n        if self.user_transcript:\n            prev = self.user_transcript[-1]\n            first = new_segments[0]\n    \n            if (\n                prev[\"person\"] == first[\"person\"]\n                and first[\"start\"] - prev[\"end\"] < 1.2\n            ):\n                prev[\"end\"] = first[\"end\"]\n                prev[\"text\"] += \" \" + first[\"text\"]\n                new_segments = new_segments[1:]\n    \n        self.user_transcript.extend(new_segments)\n        self.last_emitted_end_sec = self.user_transcript[-1][\"end\"]\n    \n        print(\n            f\"[LOG] User transcript updated | \"\n            f\"segments_added={len(new_segments)} | \"\n            f\"total={len(self.user_transcript)}\"\n        )\n\n    def finalize_remaining_segments(self):\n        print(\"[LOG] Finalizing remaining segments\")\n    \n        if self.aligned_segments:\n            for seg in self.aligned_segments:\n                seg[\"text\"] = self.deduplicate_text(seg[\"text\"])\n                self.final_transcript.append(seg)\n    \n            self.aligned_segments = []\n    \n        self.build_user_transcript(force_rebuild=True)\n    \n        print(f\"[LOG] Final transcript turns={len(self.final_transcript)}\")\n\n    def reset_batch_state(self):\n        self.audio = None\n        self.asr_segments = []\n        self.embeddings = []\n    \n        self.aligned_segments = []\n        self.merged_segments = []\n        self.final_transcript = []\n\n    def reset_session(self):\n        self.total_audio_sec = 0.0\n    \n        self.memory_bank = {}\n        self.speaker_groups = {}\n    \n        self.user_transcript = []\n\n        for attr in [\n            \"speaker_model_frozen\",\n            \"last_memory_update_idx\",\n            \"last_collapse_time_sec\",\n            \"speaker_to_person\",\n            \"next_person_id\",\n        ]:\n            if hasattr(self, attr):\n                delattr(self, attr)\n    \n        self.reset_batch_state()\n\n    def run(self, whisper_model, embedder):\n        print(\"[LOG] Starting pipeline step\")\n    \n        if self.audio is None:\n            return\n    \n        self.run_asr(whisper_model)\n        self.extract_embeddings(embedder)\n    \n        if not hasattr(self, \"speaker_model_frozen\"):\n            self.cluster_speakers()\n    \n        self.update_memory_bank()\n        self.align_segments()\n    \n        finalize_cutoff = (\n            self.total_audio_sec - 2.0\n            if self.total_audio_sec > 2.0\n            else None\n        )\n    \n        if finalize_cutoff is not None:\n            self.merge_segments(finalize_before_sec=finalize_cutoff)\n    \n        self.collapse_speakers()\n        self.apply_speaker_collapse()\n        self.build_user_transcript()\n    \n        print(\"[LOG] Pipeline step complete\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T08:02:28.657273Z","iopub.execute_input":"2026-01-09T08:02:28.657669Z","iopub.status.idle":"2026-01-09T08:02:28.827583Z","shell.execute_reply.started":"2026-01-09T08:02:28.657645Z","shell.execute_reply":"2026-01-09T08:02:28.826932Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class TalkNoteHealthMonitor:\n    def __init__(self, window_sec: float = 60.0):\n        print(\"[LOG] Initializing TalkNoteHealthMonitor\")\n\n        self.window_sec = window_sec\n\n        print(\"[LOG] Health monitor ready\")\n\n    def detect_temporal_overlap_zones(self, final_transcript):\n        zones = []\n\n        if not final_transcript:\n            return zones\n\n        segs = sorted(final_transcript, key=lambda x: x[\"start\"])\n\n        for i in range(len(segs)):\n            for j in range(i + 1, len(segs)):\n                if segs[i][\"speaker\"] == segs[j][\"speaker\"]:\n                    continue\n\n                start = max(segs[i][\"start\"], segs[j][\"start\"])\n                end = min(segs[i][\"end\"], segs[j][\"end\"])\n\n                if start < end:\n                    zones.append({\n                        \"start\": start,\n                        \"end\": end,\n                        \"source\": \"temporal_overlap\",\n                        \"speakers\": [segs[i][\"speaker\"], segs[j][\"speaker\"]],\n                    })\n\n        return zones\n\n    def detect_boundary_ambiguity_zones(\n        self,\n        embeddings,\n        bci_low: float = 0.4,\n        bci_high: float = 0.75,\n    ):\n        zones = []\n\n        if not embeddings or len(embeddings) < 2:\n            return zones\n\n        for i in range(len(embeddings) - 1):\n            a = embeddings[i]\n            b = embeddings[i + 1]\n\n            bci = float(np.dot(a[\"embedding\"], b[\"embedding\"]))\n\n            if not (bci_low <= bci <= bci_high):\n                continue\n\n            if b[\"start\"] > a[\"end\"]:\n                zones.append({\n                    \"start\": a[\"end\"],\n                    \"end\": b[\"start\"],\n                    \"source\": \"boundary_ambiguity\",\n                    \"bci\": bci,\n                })\n\n        return zones\n\n    def detect_speaker_switch_zones(\n        self,\n        final_transcript,\n        delta: float = 1.0,\n    ):\n        zones = []\n\n        if not final_transcript or len(final_transcript) < 2:\n            return zones\n\n        segs = sorted(final_transcript, key=lambda x: x[\"start\"])\n\n        for i in range(len(segs) - 1):\n            a = segs[i]\n            b = segs[i + 1]\n\n            if a[\"speaker\"] == b[\"speaker\"]:\n                continue\n\n            gap = b[\"start\"] - a[\"end\"]\n\n            if abs(gap) <= delta:\n                zones.append({\n                    \"start\": a[\"end\"],\n                    \"end\": b[\"start\"],\n                    \"source\": \"speaker_switch_proximity\",\n                    \"speakers\": [a[\"speaker\"], b[\"speaker\"]],\n                })\n\n        return zones\n\n    def collect_ambiguity_zones(\n        self,\n        final_transcript,\n        embeddings,\n        bci_low: float = 0.4,\n        bci_high: float = 0.75,\n        delta: float = 1.0,\n    ):\n        zones = []\n\n        zones.extend(\n            self.detect_temporal_overlap_zones(final_transcript)\n        )\n\n        zones.extend(\n            self.detect_boundary_ambiguity_zones(\n                embeddings,\n                bci_low=bci_low,\n                bci_high=bci_high,\n            )\n        )\n\n        zones.extend(\n            self.detect_speaker_switch_zones(\n                final_transcript,\n                delta=delta,\n            )\n        )\n\n        return zones\n\n    def compute_ambiguity_coverage(\n        self,\n        ambiguous_zones,\n        total_audio_sec: float,\n        window_sec: float | None = None,\n    ):\n        if not ambiguous_zones or total_audio_sec <= 0.0:\n            return 0.0, {}\n\n        window = window_sec if window_sec is not None else self.window_sec\n        window_start = max(0.0, total_audio_sec - window)\n\n        total_ambiguous = 0.0\n        by_source = defaultdict(float)\n\n        for z in ambiguous_zones:\n            start = max(z[\"start\"], window_start)\n            end = min(z[\"end\"], total_audio_sec)\n\n            if end > start:\n                dur = end - start\n                total_ambiguous += dur\n                by_source[z[\"source\"]] += dur\n\n        coverage = total_ambiguous / max(window, 1e-6)\n        by_source_norm = {\n            src: dur / max(window, 1e-6)\n            for src, dur in by_source.items()\n        }\n\n        return coverage, by_source_norm\n\n    def compute_memory_stability(self, memory_bank):\n        sims = []\n\n        for spk in memory_bank.values():\n            sims.extend(spk.get(\"sim_history\", []))\n\n        if not sims:\n            return 0.0\n\n        return float(np.mean(sims))\n\n    def compute_boundary_consistency(self, embeddings):\n        if not embeddings or len(embeddings) < 2:\n            return 0.0\n\n        bci_vals = [\n            float(np.dot(\n                embeddings[i][\"embedding\"],\n                embeddings[i + 1][\"embedding\"]\n            ))\n            for i in range(len(embeddings) - 1)\n        ]\n\n        if not bci_vals:\n            return 0.0\n\n        return float(np.mean(bci_vals))\n\n    def compute_health_metrics(\n        self,\n        memory_bank,\n        embeddings,\n        ambiguity_coverage: float,\n    ):\n        memory_stability = self.compute_memory_stability(memory_bank)\n        boundary_consistency = self.compute_boundary_consistency(embeddings)\n\n        high_instability = (\n            boundary_consistency < 0.6 and\n            ambiguity_coverage > 0.05\n        )\n\n        return {\n            \"memory_stability\": memory_stability,\n            \"boundary_consistency\": boundary_consistency,\n            \"ambiguity_coverage\": ambiguity_coverage,\n            \"high_instability\": high_instability,\n        }\n\n    def evaluate(\n        self,\n        *,\n        final_transcript,\n        embeddings,\n        memory_bank,\n        total_audio_sec: float,\n        bci_low: float = 0.4,\n        bci_high: float = 0.75,\n        delta: float = 1.0,\n    ):\n        ambiguous_zones = self.collect_ambiguity_zones(\n            final_transcript=final_transcript,\n            embeddings=embeddings,\n            bci_low=bci_low,\n            bci_high=bci_high,\n            delta=delta,\n        )\n\n        ambiguity_coverage, coverage_by_source = (\n            self.compute_ambiguity_coverage(\n                ambiguous_zones=ambiguous_zones,\n                total_audio_sec=total_audio_sec,\n            )\n        )\n\n        metrics = self.compute_health_metrics(\n            memory_bank=memory_bank,\n            embeddings=embeddings,\n            ambiguity_coverage=ambiguity_coverage,\n        )\n\n        return {\n            \"ambiguous_zones\": ambiguous_zones,\n            \"ambiguity_coverage\": ambiguity_coverage,\n            \"coverage_by_source\": coverage_by_source,\n            \"metrics\": metrics,\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T08:02:28.829250Z","iopub.execute_input":"2026-01-09T08:02:28.829590Z","iopub.status.idle":"2026-01-09T08:02:28.859399Z","shell.execute_reply.started":"2026-01-09T08:02:28.829557Z","shell.execute_reply":"2026-01-09T08:02:28.858787Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"METRIC_DOCS = {\n    \"memory_stability\": \"Mean cosine similarity observed during memory updates\",\n    \"boundary_consistency\": \"Mean cosine similarity between adjacent embeddings\",\n    \"ambiguity_coverage\": \"Fraction of recent audio marked ambiguous\",\n    \"high_instability\": \"Flag for low BCI + high ambiguity\"\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T08:02:28.860283Z","iopub.execute_input":"2026-01-09T08:02:28.860585Z","iopub.status.idle":"2026-01-09T08:02:28.881620Z","shell.execute_reply.started":"2026-01-09T08:02:28.860553Z","shell.execute_reply":"2026-01-09T08:02:28.881077Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def estimate_num_speakers(X, k_min=2, k_max=8):\n    best_k = k_min\n    best_score = -1.0\n\n    upper = min(k_max, len(X) - 1)\n\n    for k in range(k_min, upper + 1):\n        labels = SpectralClustering(\n            n_clusters=k,\n            affinity=\"nearest_neighbors\",\n            assign_labels=\"kmeans\",\n            random_state=42,\n        ).fit_predict(X)\n\n        score = silhouette_score(X, labels, metric=\"cosine\")\n        if score > best_score:\n            best_score = score\n            best_k = k\n\n    return best_k","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T08:02:28.882476Z","iopub.execute_input":"2026-01-09T08:02:28.882756Z","iopub.status.idle":"2026-01-09T08:02:28.896030Z","shell.execute_reply.started":"2026-01-09T08:02:28.882731Z","shell.execute_reply":"2026-01-09T08:02:28.895359Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"print(\"[LOG] Loading Whisper ASR model\")\nasr_model = whisper.load_model(\"base\", device=DEVICE)\n\nprint(\"[LOG] Loading ECAPA-TDNN embedder\")\nembedder = EncoderClassifier.from_hparams(\n    source=\"speechbrain/spkrec-ecapa-voxceleb\",\n    run_opts={\"device\": DEVICE}\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T08:02:28.896914Z","iopub.execute_input":"2026-01-09T08:02:28.897531Z","iopub.status.idle":"2026-01-09T08:02:34.455139Z","shell.execute_reply.started":"2026-01-09T08:02:28.897508Z","shell.execute_reply":"2026-01-09T08:02:34.454333Z"}},"outputs":[{"name":"stdout","text":"[LOG] Loading Whisper ASR model\n","output_type":"stream"},{"name":"stderr","text":"100%|████████████████████████████████████████| 139M/139M [00:01<00:00, 131MiB/s]\n","output_type":"stream"},{"name":"stdout","text":"[LOG] Loading ECAPA-TDNN embedder\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"hyperparams.yaml: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f1247b2245d477f956999bd3182044d"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/speechbrain/utils/torch_audio_backend.py:57: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n  available_backends = torchaudio.list_audio_backends()\n/usr/local/lib/python3.12/dist-packages/speechbrain/utils/autocast.py:188: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"embedding_model.ckpt:   0%|          | 0.00/83.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3de7a55cc3e14e84a30d35b715132fa6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"mean_var_norm_emb.ckpt:   0%|          | 0.00/1.92k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75cc5d878545407893fe0bda51320e4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"classifier.ckpt:   0%|          | 0.00/5.53M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78291135efa4434e8936ca0bc5b856f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"label_encoder.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4cfa6f6142e4f0a9e722933c3b719f5"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"from glob import glob\n\nAUDIO_DIR = \"/kaggle/input/voxconverse-dataset/voxconverse_dev_wav/audio\"\nMAX_FILES = 3\n\naudio_files = sorted(glob(os.path.join(AUDIO_DIR, \"*.wav\")))[:MAX_FILES]\n\nprint(f\"[LOG] Found {len(audio_files)} audio files\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T08:02:34.456092Z","iopub.execute_input":"2026-01-09T08:02:34.456379Z","iopub.status.idle":"2026-01-09T08:02:34.475044Z","shell.execute_reply.started":"2026-01-09T08:02:34.456346Z","shell.execute_reply":"2026-01-09T08:02:34.474539Z"}},"outputs":[{"name":"stdout","text":"[LOG] Found 3 audio files\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"print(\"[LOG] Creating TalkNotePipeline object\")\n\npipeline = TalkNotePipeline(\n    session_id=\"test_session\",\n    run_dir=\".\",\n    sample_rate=SAMPLE_RATE,\n    speaker_estimator=estimate_num_speakers\n)\n\nprint(\"[LOG] Creating TalkNoteHeMonitor object\")\n\nmonitor = TalkNoteHealthMonitor(window_sec=60.0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T08:02:34.475795Z","iopub.execute_input":"2026-01-09T08:02:34.476053Z","iopub.status.idle":"2026-01-09T08:02:34.480444Z","shell.execute_reply.started":"2026-01-09T08:02:34.476031Z","shell.execute_reply":"2026-01-09T08:02:34.479803Z"}},"outputs":[{"name":"stdout","text":"[LOG] Creating TalkNotePipeline object\n[LOG] Initializing TalkNotePipeline\n[LOG] Initialization complete | session=test_session\n[LOG] Creating TalkNoteHeMonitor object\n[LOG] Initializing TalkNoteHealthMonitor\n[LOG] Health monitor ready\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"print(\"[LOG] Starting offline validation run\")\n\nTEST_FILES = audio_files[:1]\n\nfor idx, audio_path in enumerate(TEST_FILES):\n    print(f\"\\n[LOG] File {idx+1}/{len(TEST_FILES)}: {os.path.basename(audio_path)}\")\n\n    # Reset pipeline cleanly\n    pipeline.reset_session()\n\n    # Load full audio\n    audio, sr = librosa.load(audio_path, sr=SAMPLE_RATE, mono=True)\n\n    CHUNK_SAMPLES = int(SAMPLE_RATE * ASR_CHUNK_SEC)\n\n    # Stream audio in chunks\n    for start in range(0, len(audio), CHUNK_SAMPLES):\n        chunk = audio[start : start + CHUNK_SAMPLES]\n        pipeline.load_audio(chunk)\n        pipeline.run(whisper_model=asr_model, embedder=embedder)\n\n    pipeline.finalize_remaining_segments()\n\n    # -------- Evaluation --------\n    health = monitor.evaluate(\n        final_transcript=pipeline.final_transcript,\n        embeddings=pipeline.embeddings,\n        memory_bank=pipeline.memory_bank,\n        total_audio_sec=pipeline.total_audio_sec,\n    )\n\n    # -------- Inspection --------\n    print(\"[RESULT] Turns:\", len(pipeline.user_transcript))\n    print(\"[RESULT] Speakers:\", {\n        seg[\"person\"] for seg in pipeline.user_transcript\n    })\n\n    print(\"[HEALTH]\")\n    for k, v in health[\"metrics\"].items():\n        print(f\"  {k}: {v}\")\n\n    print(\"[AMBIGUITY]\")\n    print(\"  zones:\", len(health[\"ambiguous_zones\"]))\n    print(\"  coverage:\", round(health[\"ambiguity_coverage\"], 4))\n\nprint(\"FINISH\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T08:02:34.482052Z","iopub.execute_input":"2026-01-09T08:02:34.482268Z","iopub.status.idle":"2026-01-09T08:02:50.918747Z","shell.execute_reply.started":"2026-01-09T08:02:34.482248Z","shell.execute_reply":"2026-01-09T08:02:50.918000Z"}},"outputs":[{"name":"stdout","text":"[LOG] Starting offline validation run\n\n[LOG] File 1/1: abjxc.wav\n[LOG] Audio chunk loaded | samples=160000 | offset=0.00s\n[LOG] Starting pipeline step\n[LOG] Entering run_asr\nDetected language: English\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1000/1000 [00:01<00:00, 932.12frames/s]\n","output_type":"stream"},{"name":"stdout","text":"[LOG] ASR batch completed | segments=2\n[LOG] Entering extract_embeddings\n[LOG] Embeddings appended | total=1\n[LOG] Entering cluster_speakers (warm-up mode)\n[LOG] Insufficient embeddings for initial clustering\n[LOG] Entering update_memory_bank\n[LOG] Speaker model not frozen — skipping memory update\n[LOG] Entering align_segments\n[LOG] Memory bank empty — cannot assign speakers\n[LOG] Entering merge_segments\n[LOG] Entering collapse_speakers\n[LOG] Pipeline step complete\n[LOG] Audio chunk loaded | samples=160000 | offset=10.00s\n[LOG] Starting pipeline step\n[LOG] Entering run_asr\nDetected language: English\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1000/1000 [00:00<00:00, 3100.20frames/s]\n","output_type":"stream"},{"name":"stdout","text":"[LOG] ASR batch completed | segments=2\n[LOG] Entering extract_embeddings\n[LOG] Embeddings appended | total=2\n[LOG] Entering cluster_speakers (warm-up mode)\n[LOG] Insufficient embeddings for initial clustering\n[LOG] Entering update_memory_bank\n[LOG] Speaker model not frozen — skipping memory update\n[LOG] Entering align_segments\n[LOG] Memory bank empty — cannot assign speakers\n[LOG] Entering merge_segments\n[LOG] Entering collapse_speakers\n[LOG] Pipeline step complete\n[LOG] Audio chunk loaded | samples=160000 | offset=20.00s\n[LOG] Starting pipeline step\n[LOG] Entering run_asr\nDetected language: English\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1000/1000 [00:00<00:00, 3083.44frames/s]\n","output_type":"stream"},{"name":"stdout","text":"[LOG] ASR batch completed | segments=3\n[LOG] Entering extract_embeddings\n[LOG] Embeddings appended | total=3\n[LOG] Entering cluster_speakers (warm-up mode)\n[LOG] Insufficient embeddings for initial clustering\n[LOG] Entering update_memory_bank\n[LOG] Speaker model not frozen — skipping memory update\n[LOG] Entering align_segments\n[LOG] Memory bank empty — cannot assign speakers\n[LOG] Entering merge_segments\n[LOG] Entering collapse_speakers\n[LOG] Pipeline step complete\n[LOG] Audio chunk loaded | samples=160000 | offset=30.00s\n[LOG] Starting pipeline step\n[LOG] Entering run_asr\nDetected language: English\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1000/1000 [00:00<00:00, 2738.20frames/s]\n","output_type":"stream"},{"name":"stdout","text":"[LOG] ASR batch completed | segments=3\n[LOG] Entering extract_embeddings\n[LOG] Embeddings appended | total=4\n[LOG] Entering cluster_speakers (warm-up mode)\n[LOG] Insufficient embeddings for initial clustering\n[LOG] Entering update_memory_bank\n[LOG] Speaker model not frozen — skipping memory update\n[LOG] Entering align_segments\n[LOG] Memory bank empty — cannot assign speakers\n[LOG] Entering merge_segments\n[LOG] Entering collapse_speakers\n[LOG] Pipeline step complete\n[LOG] Audio chunk loaded | samples=160000 | offset=40.00s\n[LOG] Starting pipeline step\n[LOG] Entering run_asr\nDetected language: English\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1000/1000 [00:00<00:00, 2481.05frames/s]\n","output_type":"stream"},{"name":"stdout","text":"[LOG] ASR batch completed | segments=5\n[LOG] Entering extract_embeddings\n[LOG] Embeddings appended | total=5\n[LOG] Entering cluster_speakers (warm-up mode)\n[LOG] Insufficient embeddings for initial clustering\n[LOG] Entering update_memory_bank\n[LOG] Speaker model not frozen — skipping memory update\n[LOG] Entering align_segments\n[LOG] Memory bank empty — cannot assign speakers\n[LOG] Entering merge_segments\n[LOG] Entering collapse_speakers\n[LOG] Pipeline step complete\n[LOG] Audio chunk loaded | samples=160000 | offset=50.00s\n[LOG] Starting pipeline step\n[LOG] Entering run_asr\nDetected language: English\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1000/1000 [00:00<00:00, 2648.69frames/s]\n","output_type":"stream"},{"name":"stdout","text":"[LOG] ASR batch completed | segments=4\n[LOG] Entering extract_embeddings\n[LOG] Embeddings appended | total=6\n[LOG] Entering cluster_speakers (warm-up mode)\n[LOG] Insufficient embeddings for initial clustering\n[LOG] Entering update_memory_bank\n[LOG] Speaker model not frozen — skipping memory update\n[LOG] Entering align_segments\n[LOG] Memory bank empty — cannot assign speakers\n[LOG] Entering merge_segments\n[LOG] Entering collapse_speakers\n[LOG] Pipeline step complete\n[LOG] Audio chunk loaded | samples=134016 | offset=60.00s\n[LOG] Starting pipeline step\n[LOG] Entering run_asr\nDetected language: English\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 837/837 [00:00<00:00, 2882.28frames/s]","output_type":"stream"},{"name":"stdout","text":"[LOG] ASR batch completed | segments=2\n[LOG] Entering extract_embeddings\n[LOG] Embeddings appended | total=6\n[LOG] Entering cluster_speakers (warm-up mode)\n[LOG] Insufficient embeddings for initial clustering\n[LOG] Entering update_memory_bank\n[LOG] Speaker model not frozen — skipping memory update\n[LOG] Entering align_segments\n[LOG] Memory bank empty — cannot assign speakers\n[LOG] Entering merge_segments\n[LOG] Entering collapse_speakers\n[LOG] Pipeline step complete\n[LOG] Finalizing remaining segments\n[LOG] Rebuilding user transcript from scratch\n[LOG] Final transcript turns=0\n[RESULT] Turns: 0\n[RESULT] Speakers: set()\n[HEALTH]\n  memory_stability: 0.0\n  boundary_consistency: 0.8562325716018677\n  ambiguity_coverage: 0.0\n  high_instability: False\n[AMBIGUITY]\n  zones: 0\n  coverage: 0.0\nFINISH\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":10}]}